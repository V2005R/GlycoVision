{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import print_function, division\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils import data\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport torchvision.models as models\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import io, transform\nimport torch.utils.data as data_utils\nfrom PIL import Image, ImageFile\nimport json\nfrom torch.optim import lr_scheduler\nimport time\nimport os\nimport argparse\nimport copy\nimport pandas as pd\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport cv2\nimport sklearn\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score\nimport time\nfrom tqdm import tqdm_notebook","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!kaggle competitions download -c aptos2019-blindness-detection","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip aptos2019-blindness-detection.zip -d aptos2019","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_csv = pd.read_csv('/content/aptos2019/train.csv')\ntest_csv = pd.read_csv('/content/aptos2019/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Train Size = {}'.format(len(train_csv)))\nprint('Public Test Size = {}'.format(len(test_csv)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_csv.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncounts = train_csv['diagnosis'].value_counts()\nclass_list = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferate']\n\nfor i, x in enumerate(class_list):\n    counts[x] = counts.pop(i)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=counts.index, y=counts.values, alpha=0.8, palette='bright')\nplt.title('Distribution of Output Classes')\nplt.xlabel('Target Classes', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nbase_dir = '/content/aptos2019'\n\ntrain_imgs = os.listdir(os.path.join(base_dir, 'train_images'))\n\nfig = plt.figure(figsize=(30, 6))\nfor idx, img in enumerate(np.random.choice(train_imgs, 16)):\n    ax = fig.add_subplot(2, 16 // 2, idx + 1, xticks=[], yticks=[])\n    im = Image.open(os.path.join(base_dir, \"train_images\", img))\n    plt.imshow(im)\n    lab = train_csv.loc[train_csv['id_code'] == img.split('.')[0], 'diagnosis'].values[0]\n    ax.set_title('Severity: %s' % lab)\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fig = plt.figure(figsize=(30, 6))\ntest_imgs = os.listdir(base_dir+\"/test_images\")\nfor idx, img in enumerate(np.random.choice(test_imgs, 16)):\n    ax = fig.add_subplot(2, 16//2, idx+1, xticks=[], yticks=[])\n    im = Image.open(base_dir+\"/test_images/\" + img)\n    plt.imshow(im)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CreateDataset(Dataset):\n    def __init__(self, df_data, data_dir = '../input/', transform=None):\n        super().__init__()\n        self.df = df_data.values\n        self.data_dir = data_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        img_name,label = self.df[index]\n        img_path = os.path.join(self.data_dir, img_name+'.png')\n        image = cv2.imread(img_path)\n        if self.transform is not None:\n            image = self.transform(image)\n        return image, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transforms = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((256, 256)),\n    transforms.RandomHorizontalFlip(p=0.4),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_transforms = transforms.Compose([transforms.Resize(256),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_path = \"/content/aptos2019/train_images\"\ntest_path = \"/content/aptos2019/test.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = CreateDataset(df_data=train_csv, data_dir=train_path, transform=train_transforms)\ntest_data = CreateDataset(df_data=test_csv, data_dir=test_path, transform=test_transforms)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"valid_size = 0.2\nnum_train = len(train_data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nsplit = int(np.floor(valid_size * num_train))\ntrain_idx, valid_idx = indices[split:], indices[:split]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainloader = torch.utils.data.DataLoader(train_data, batch_size=64,sampler=train_sampler)\nvalidloader = torch.utils.data.DataLoader(train_data, batch_size=64, sampler=valid_sampler)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=64)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"training examples contain : {len(train_data)}\")\nprint(f\"testing examples contain : {len(test_data)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(trainloader))\n\nprint(f\"Image shape : {images.shape}\")\nprint(f\"Label shape : {labels.shape}\")\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision\nimport matplotlib.pyplot as plt\nimport numpy as np\ngrid = torchvision.utils.make_grid(images, nrow=30, padding=7)\ngrid = grid.cpu()\nplt.figure(figsize=(20, 20))\nplt.imshow(np.transpose(grid, (1, 2, 0)))\nprint('labels:', labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n\nimages, labels = next(iter(trainloader))\nout = torchvision.utils.make_grid(images)\nimshow(out, title=[class_names[x] for x in labels])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport os\nimport shutil\nimport random\n\ntrain = '/content/aptos2019/train_images'\ntest = '/content/aptos2019/test_images'\nos.makedirs('test' , exist_ok=True)\nos.makedirs('train' , exist_ok=True)\nos.makedirs('val' , exist_ok=True)\n\nt_df = pd.read_csv('/content/aptos2019/train.csv')\n\ntrain_df = t_df.sample(frac = 0.80)\ntest_df = t_df.drop(train_df.index)\ntest_df.reset_index(drop=True, inplace=True)\n\nval_df = test_df.sample(frac = 0.45)\ntest_df = test_df.drop(val_df.index)\ntest_df.reset_index(drop=True, inplace=True)\nval_df.reset_index(drop=True, inplace=True)\n\ntrain_df.sort_values(by='diagnosis', inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df.sort_values(by='diagnosis', inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\nval_df.sort_values(by='diagnosis', inplace=True)\nval_df.reset_index(drop=True, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df['diagnosis'].value_counts().plot(kind='pie' , labels = list(test_df['diagnosis'].value_counts()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_df['diagnosis'].value_counts().plot(kind='pie' , labels = list(val_df['diagnosis'].value_counts()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(5):\n  imgs = train_df[train_df['diagnosis'] == i]['id_code'].to_list()\n  os.makedirs(os.path.join('train', str(i)), exist_ok=True)\n  for j in imgs:\n    shutil.copy(os.path.join(train, j+'.png'), os.path.join(f'train/{i}', str(j)+'.png'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(5):\n  imgs = test_df[test_df['diagnosis'] == i]['id_code'].to_list()\n  os.makedirs(os.path.join('test', str(i)), exist_ok=True)\n  for j in imgs:\n    shutil.copy(os.path.join(train, j+'.png'), os.path.join(f'test/{i}', str(j)+'.png'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(5):\n  imgs = val_df[val_df['diagnosis'] == i]['id_code'].to_list()\n  os.makedirs(os.path.join('val', str(i)), exist_ok=True)\n  for j in imgs:\n    shutil.copy(os.path.join(train, j+'.png'), os.path.join(f'val/{i}', str(j)+'.png'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ssl\nssl._create_default_https_context = ssl._create_unverified_context\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pretrainedmodels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models, datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom pretrainedmodels import inceptionresnetv2\n\nmodel = inceptionresnetv2(num_classes=1000, pretrained='imagenet')\n\nlayers = list(model.children())\nfor layer in layers[:-50]:\n    for param in layer.parameters():\n        param.requires_grad = False\n\nnum_classes = 5\nin_features = model.last_linear.in_features\nmodel.last_linear = nn.Linear(in_features, num_classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.00001)\ntransform = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.RandomHorizontalFlip(p=0.4),\n    transforms.RandomVerticalFlip(p=0.4),\n    transforms.RandomRotation(degrees=(0, 10)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = datasets.ImageFolder(root='/content/train', transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nval_dataset = datasets.ImageFolder(root='/content/val', transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nbest_val_accuracy = 0.0\nbest_model_path = \"best_model.pth\"  # Path to save the best model\n\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Training...\")\n\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n        _, predicted = torch.max(outputs, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n\n        if (batch_idx + 1) % 10 == 0:  # Print every 10 batches\n            train_accuracy = 100 * correct_train / total_train\n            print(f\"Batch [{batch_idx + 1}/{len(train_loader)}] - \"\n                  f\"Loss: {loss.item():.4f}, \"\n                  f\"Train Accuracy: {train_accuracy:.2f}%\")\n\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = 100 * correct_train / total_train\n\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] - Validating...\")\n\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_loss = val_loss / len(val_loader)\n    val_accuracy = 100 * correct_val / total_val\n\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"Best model saved with validation accuracy: {best_val_accuracy:.2f}%\")\n\n    print(f\"Epoch [{epoch + 1}/{num_epochs}] - \"\n          f\"Train Loss: {train_loss:.4f}, \"\n          f\"Train Accuracy: {train_accuracy:.2f}%, \"\n          f\"Val Loss: {val_loss:.4f}, \"\n          f\"Val Accuracy: {val_accuracy:.2f}%, \"\n          f\"Time: {time.time() - start_time:.2f} seconds\\n\")\n\nprint(\"Training complete. Best model saved at:\", best_model_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model, 'model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nimport pretrainedmodels\nfrom sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n\nmodel = pretrainedmodels.__dict__['inceptionresnetv2'](num_classes=1000, pretrained='imagenet')\nnum_classes = 5\n\nin_features = model.last_linear.in_features\nmodel.last_linear = nn.Linear(in_features, num_classes)\n\nmodel.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\n\ntransform = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntest_dir = '/content/test'\ntest_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ndef evaluate(model, test_loader, criterion, device):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item() * images.size(0)\n\n            # Calculate accuracy\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            all_preds.extend(predicted.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    avg_loss = test_loss / total\n    accuracy = correct / total\n\n    return avg_loss, accuracy, all_preds, all_labels\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\ncriterion = nn.CrossEntropyLoss()\ntest_loss, test_accuracy, all_preds, all_labels = evaluate(model, test_loader, criterion, device)\n\nprint(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy * 100:.2f}%\")\n\nprecision = precision_score(all_labels, all_preds, average='weighted')\nrecall = recall_score(all_labels, all_preds, average='weighted')\nf1 = f1_score(all_labels, all_preds, average='weighted')\nkappa_score = cohen_kappa_score(all_labels, all_preds)\n\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Cohen's Kappa Score: {kappa_score:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}